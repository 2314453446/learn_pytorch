{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
    "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#模型是指 参数 与输入输出之间的关系\n",
    "def model(t_u,w,b):\n",
    "    return w * t_u + b\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 损失函数是用来定义 预测值和实际值之间的相对关系\n",
    "def loss_fn(t_p,t_c):\n",
    "    squared_diffs = (t_p-t_c)**2\n",
    "    return squared_diffs.mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['ASGD',\n 'Adadelta',\n 'Adagrad',\n 'Adam',\n 'AdamW',\n 'Adamax',\n 'LBFGS',\n 'NAdam',\n 'Optimizer',\n 'RAdam',\n 'RMSprop',\n 'Rprop',\n 'SGD',\n 'SparseAdam',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '_functional',\n '_multi_tensor',\n 'lr_scheduler',\n 'swa_utils']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0],requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params],lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#什么是优化器\n",
    "#优化器的输入为参数w,b,学习率，梯度。输出为更新后的params ,\n",
    "#相当于在 slf1中的这行代码params = params - leaning_rate*grad\n",
    "#自动梯度下降中的params包含.grad方法，即params中已经包含了梯度对象，我们无须重新定义\n",
    "#所以当调用optimizer时只需要传入params 和 lr\n",
    "#optimizer.step()即利用当前梯度对于参数进行一次更新\n",
    "t_p = model(t_u,*params)\n",
    "loss = loss_fn(t_p,t_c)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer,params,t_u,t_c):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        t_p = model(t_u,*params)\n",
    "        loss = loss_fn(t_p,t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('epoch %d ,loss %f'% (epoch,float(loss)))\n",
    "\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500 ,loss 7.860120\n",
      "epoch 1000 ,loss 3.828538\n",
      "epoch 1500 ,loss 3.092191\n",
      "epoch 2000 ,loss 2.957698\n",
      "epoch 2500 ,loss 2.933134\n",
      "epoch 3000 ,loss 2.928648\n",
      "epoch 3500 ,loss 2.927830\n",
      "epoch 4000 ,loss 2.927679\n",
      "epoch 4500 ,loss 2.927652\n",
      "epoch 5000 ,loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([  5.3671, -17.3012], requires_grad=True)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0],requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params],lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs= 5000,\n",
    "    optimizer= optimizer,\n",
    "    params = params,\n",
    "    t_u =t_un,\n",
    "    t_c = t_c\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500 ,loss 7.612900\n",
      "epoch 1000 ,loss 3.086700\n",
      "epoch 1500 ,loss 2.928579\n",
      "epoch 2000 ,loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([  0.5367, -17.3021], requires_grad=True)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam 优化器全称，Adaptive Moment Estimation，自适应矩估计，会自动调整步长\n",
    "\n",
    "params = torch.tensor([1.0,0.0],requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params],lr= learning_rate)\n",
    "\n",
    "training_loop(n_epochs=2000,\n",
    "              optimizer=optimizer,\n",
    "              params=params,\n",
    "              t_u=t_u,\n",
    "              t_c= t_c)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2*n_samples)\n",
    "#验证数据集划分 20%\n",
    "\n",
    "n_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 6,  8,  3,  4,  2,  5,  7,  1,  9,  0, 10]),\n tensor([6, 8, 3, 4, 2, 5, 7, 1, 9]),\n tensor([ 0, 10]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = torch.randperm(n_samples)\n",
    "# random permutation随机排序，范围为，(0,samples-1)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "shuffled_indices,train_indices,val_indices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#tensor 中 可以直接传入索引，根据索引值获取参数张量\n",
    "train_t_u =t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 *train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u,val_t_u,\n",
    "                  train_t_c,val_t_c):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        train_t_p = model(train_t_u,*params)\n",
    "        train_loss = loss_fn(train_t_p,train_t_c)\n",
    "\n",
    "        val_t_p = model(val_t_u,*params)\n",
    "        val_loss = loss_fn(val_t_p,val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <=3 or epoch %500 ==0:\n",
    "            print(f\"Epoch{epoch},train loss {train_loss.item():.4f},\"\n",
    "                  f\"Validation loss {val_loss.item():.4f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "# f\"\"的作用为在字符串中嵌入表达式 {}中写入变量名\n",
    "# .item()将张量转换成标量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1,train loss 74.8975,Validation loss 104.9652\n",
      "Epoch2,train loss 34.0469,Validation loss 56.6435\n",
      "Epoch3,train loss 27.6024,Validation loss 47.3689\n",
      "Epoch500,train loss 7.3489,Validation loss 15.1187\n",
      "Epoch1000,train loss 3.7854,Validation loss 7.2841\n",
      "Epoch1500,train loss 3.1280,Validation loss 4.8208\n",
      "Epoch2000,train loss 3.0067,Validation loss 3.9292\n",
      "Epoch2500,train loss 2.9844,Validation loss 3.5769\n",
      "Epoch3000,train loss 2.9802,Validation loss 3.4312\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([  5.1394, -16.1405], requires_grad=True)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0],requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params],lr=learning_rate)\n",
    "\n",
    "training_loop(n_epochs=3000,\n",
    "              optimizer=optimizer,\n",
    "              params=params,\n",
    "              train_t_u=train_t_un,\n",
    "              train_t_c=train_t_c,\n",
    "              val_t_u=val_t_un,\n",
    "              val_t_c=val_t_c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs,optimizer,params,train_t_u,val_t_u,\n",
    "                  train_t_c,val_t_c):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        train_t_p = model(train_t_u,*params)\n",
    "        train_loss = loss_fn(train_t_p,train_t_c)\n",
    "\n",
    "        with torch.no_grad(): #torch.no_grad()就是不利用验证集进行梯度更新\n",
    "            val_t_p = model(val_t_u,*params)\n",
    "            val_loss = loss_fn(val_t_p,val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "            # assert 断言 val_loss.requires_grad == False，如果不是就会报错AssertionError\n",
    "\n",
    "        optimizer.zero_grad() # 清空上次运行的优化器的梯度\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def calc_forward(t_u,t_c, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u,*params)\n",
    "        loss = loss_fn(t_p,t_c)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
